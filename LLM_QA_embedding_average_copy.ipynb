{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import wordninja\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type=\"fact_checking\"\n",
    "rag_type = \"xRAG\"\n",
    "max_new_tokens = 256\n",
    "layer_num = 2\n",
    "icl_sample_num = 0\n",
    "model_name = \"Mistral-7B2\"\n",
    "dataset_name = \"Strategyqa\"\n",
    "data_confs_path = \"configs/data.yaml\"\n",
    "data_confs = load_yaml_conf(data_confs_path)\n",
    "train_sample_num = 100\n",
    "model_confs_path = \"configs/model.yaml\"\n",
    "\n",
    "model_confs = load_yaml_conf(model_confs_path)\n",
    "# train_data = load_dataset(data_confs[dataset_name]['data_train_path'])\n",
    "\n",
    "# test_data = load_dataset(data_confs[dataset_name]['data_test_path'])\n",
    "# train_data = train_data[:train_sample_num]\n",
    "test_sample_num = data_confs[dataset_name]['test_sample_num']\n",
    "template = model_confs[model_name]['template']\n",
    "# question = data_confs[dataset_name]['question']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "res_file = 'res/' + dataset_name+\"_\" + rag_type+\"_\"+model_name+str(int(t))+'.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d56652e2264f759cef6e7a045197a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_path = \"/data1/whl/xRAG/pretrained_model/pretrain/last\"\n",
    "from src.model import (\n",
    "    XMistralForCausalLM,\n",
    "    XMistralConfig,\n",
    "    XMixtralForCausalLM,\n",
    "    XMixtralConfig,\n",
    "    SFR,\n",
    ")\n",
    "model = XMistralForCausalLM.from_pretrained(llm_path)\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     llm_path, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "model.layers.28.mlp.gate_proj.weight\n",
      "model.layers.28.mlp.up_proj.weight\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "model.layers.28.input_layernorm.weight\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "model.layers.29.mlp.gate_proj.weight\n",
      "model.layers.29.mlp.up_proj.weight\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "model.layers.29.input_layernorm.weight\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "model.layers.30.mlp.gate_proj.weight\n",
      "model.layers.30.mlp.up_proj.weight\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "model.layers.30.input_layernorm.weight\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "model.layers.31.input_layernorm.weight\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "# 打印所有参数名称\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '</s>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "# special_token = \"<xRAG>\"\n",
    "# special_tokens_dict = {'additional_special_tokens': ['<xRAG>']}\n",
    "# tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "\n",
    "# # special_tokens_dict = {'additional_special_tokens': ['<xRAG>']}\n",
    "# # num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# # 打印分词器中添加的特殊标记\n",
    "print(tokenizer.all_special_tokens)\n",
    "\n",
    "# # 现在可以使用分词器对包含<xRAG>的文本进行编码\n",
    "# prompt = '<xRAG> Are more people today related to Genghis Khan than Julius Caesar?'\n",
    "# encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# # 打印编码后的结果\n",
    "# print(encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_embed_avg(inputs):\n",
    "  inputs = tokenizer(inputs, return_tensors=\"pt\").to(device)\n",
    "  if \"Qwen\"in model_name:\n",
    "      token_embed = torch.mean(model.embed_tokens(inputs.input_ids), dim=1, keepdim=True)\n",
    "  else:\n",
    "     token_embed = torch.mean(model.model.embed_tokens(inputs.input_ids), dim=1, keepdim=True)\n",
    "  return token_embed\n",
    "\n",
    "def get_tokens_embed(inputs):\n",
    "  inputs = tokenizer(inputs, return_tensors=\"pt\").to(device)\n",
    "  if \"Qwen\"in model_name:\n",
    "      token_embed = model.embed_tokens(inputs.input_ids)\n",
    "  else:\n",
    "      token_embed = model.model.embed_tokens(inputs.input_ids)\n",
    "  return token_embed\n",
    "\n",
    "def get_background(facts,dataset_name):\n",
    "    if \"hotpot\" in dataset_name.lower():\n",
    "        backgrounds = []\n",
    "        i = 0\n",
    "        for fact in facts:\n",
    "            i = i+1\n",
    "            backgrounds.append('['+str(i)+']'+\" \".join(fact[1]))\n",
    "        return \" \".join(backgrounds)\n",
    "    elif \"strategyqa\" in dataset_name.lower():\n",
    "        backgrounds = []\n",
    "        i = 0\n",
    "        for fact in facts:\n",
    "            i = i+1\n",
    "            backgrounds.append('['+str(i)+']'+fact)\n",
    "        return \" \".join(backgrounds)\n",
    "    elif \"triviaqa\" in dataset_name.lower():\n",
    "        backgrounds=facts\n",
    "        return backgrounds\n",
    "\n",
    "XRAG_TOKEN = \"<unk>\" \n",
    "def get_task_prompt(question,backgrounds,task_type,rag_type,background_shot = 0):\n",
    "    \n",
    "    if task_type == 'open_qa':\n",
    "        return {\n",
    "            \"NO\":\"Answer the questions: \"+question+\"\\n\", \n",
    "            \"RAG\": backgrounds+\" Refer to the background document and answer the questions: \"+question+\"\\n\",\n",
    "            \"xRAG\": \" \".join([XRAG_TOKEN]* background_shot) + \" \"+\"Refer to the background document and answer the questions: \"+question+\"\\n\",\n",
    "        }[rag_type]\n",
    "    elif task_type == 'fact_checking':\n",
    "        return {\n",
    "            \"NO\":\"Verify the following claims with \\\"True\\\" or \\\"False\\\": \"+question+\"\\n\", \n",
    "            \"RAG\": backgrounds+\" Refer to the background document and verify the following claims with \\\"True\\\" or \\\"False\\\": \"+question+\"\\n\",\n",
    "            \"xRAG\": \" \".join([XRAG_TOKEN]* background_shot) + \" \"+\"Refer to the background document and verify the following claims with \\\"True\\\" or \\\"False\\\": \"+question+\"\\n\",\n",
    "        }[rag_type]\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_xrag_index(input_text):\n",
    "    special_token = \"<unk>\"\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    indices = [i for i, token in enumerate(tokens) if token == special_token]\n",
    "\n",
    "    if indices:\n",
    "        print(f\"特殊 token '{special_token}' 的所有位置是: {indices}\")\n",
    "    else:\n",
    "        print(f\"特殊 token '{special_token}' 未找到\")\n",
    "    return indices\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qid': 'b8677742616fef051f00', 'term': 'Genghis Khan', 'description': 'founder and first Great Khan of the Mongol Empire', 'question': 'Are more people today related to Genghis Khan than Julius Caesar?', 'answer': True, 'facts': ['Julius Caesar had three children.', 'Genghis Khan had sixteen children.', 'Modern geneticists have determined that  out of every 200 men today has DNA that can be traced to Genghis Khan.'], 'decomposition': ['How many kids did Julius Caesar have?', 'How many kids did Genghis Khan have?', 'Is #2 greater than #1?'], 'evidence': [[[['Caesarion-2', 'Julia (daughter of Caesar)-1']], [['Alakhai Bekhi-1', 'Tolui-1'], 'no_evidence'], ['operation']], [[['Julius Caesar-75']], [['Genghis Khan-17']], ['operation']], [[['Gaius Julius Caesar-7']], [['Genghis Khan-15'], 'no_evidence'], ['no_evidence', 'operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Are more people today related to Genghis Khan than Julius Caesar?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 74, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False. It is estimated that approximately 0.5% of the global population, or about 16 million people, can trace their ancestry back to Genghis Khan. In contrast, the number of people who can trace their ancestry back to Julius Caesar is much smaller, as his lineage is limited to his direct descendants and their descendants. Therefore, it is unlikely that more people today are related to Genghis Khan than Julius Caesar.\n",
      "result is error!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "0\n",
      "1\n",
      "{'qid': '4668264df84fb7c25b39', 'term': 'The Police', 'description': 'English rock band', 'question': 'Could the members of The Police perform lawful arrests?', 'answer': False, 'facts': ['The members of The Police were musicians, not law enforcement officers.', 'Only law enforcement officers can perform lawful arrests.'], 'decomposition': ['Who can perform lawful arrests?', 'Are members of The Police also #1?'], 'evidence': [[[['Arrest-2']], [[\"Citizen's arrest-2\", 'The Police-1'], 'operation']], [[['Arrest-7', 'Law enforcement officer-13']], [['Law enforcement officer-13', 'The Police-8']]], [[['Lawful interception-28']], ['operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Could the members of The Police perform lawful arrests?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 70, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "True. The members of The Police are authorized individuals who have the power to make lawful arrests under the laws of their jurisdiction, as long as they have probable cause or a warrant. This is a standard authority granted to law enforcement officers.\n",
      "result is error!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "0\n",
      "2\n",
      "{'qid': '41450dbd623437269f8f', 'term': 'Depression (mood)', 'description': 'state of low mood and fatigue', 'question': 'Would a Monoamine Oxidase candy bar cheer up a depressed friend?', 'answer': False, 'facts': ['Depression is caused by low levels of serotonin, dopamine and norepinephrine.', 'Monoamine Oxidase breaks down neurotransmitters and lowers levels of serotonin, dopamine and norepinephrine.'], 'decomposition': ['Depression is caused by low levels of what chemicals?', 'Monoamine Oxidase has an effect on what chemicals?', 'Of the chemicals listed in both #1 and #2, does Monoamine Oxidase raise their levels?'], 'evidence': [[[['Monoamine oxidase-8']], [['Monoamine oxidase-1'], 'no_evidence'], ['no_evidence', 'operation']], [[['Depression (mood)-13']], [['Monoamine oxidase-8']], [['Serotonin-36'], 'operation']], [[['Biology of depression-1']], [['Monoamine oxidase-7']], ['operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Would a Monoamine Oxidase candy bar cheer up a depressed friend?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 73, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False. A Monoamine Oxidase (MAO) candy bar does not exist. MAO is an enzyme that breaks down certain neurotransmitters in the body, including serotonin and dopamine. Consuming foods that contain high levels of tyramine, such as aged cheeses, red wine, and certain meats, can inhibit MAO and lead to dangerous increases in neurotransmitter levels. Eating a candy bar is unlikely to have any effect on a person's mood, let alone cheer up a depressed friend. If someone is experiencing depression, it's important to encourage them to speak with a healthcare professional for appropriate treatment.\n",
      "result is right!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "1\n",
      "3\n",
      "{'qid': '7419826e4373eb1e83ff', 'term': 'Grey seal', 'description': 'species of seal', 'question': 'Would a dog respond to bell before Grey seal?', 'answer': True, 'facts': ['Grey seals have no ear flaps and their ears canals are filled with wax.', 'Grey seals hear better underwater when their ears open like a valve.', 'Dogs have sensitive ears that can hear as far as a quarter of a mile away.'], 'decomposition': [\"How sensitive is a grey seal's hearing on land?\", \"How sensitive is a dog's hearing on land?\", 'Is #2 better than #1?'], 'evidence': [[[['Pinniped-24']], [['Hearing range-11', 'Hertz-5']], ['operation']], [[['Grey seal-1'], 'no_evidence'], [['Dog-54'], 'no_evidence'], ['no_evidence', 'operation']], [[['Grey seal-1'], 'no_evidence'], [['Dog anatomy-114']], ['operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Would a dog respond to bell before Grey seal?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 68, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False. Dogs do not naturally respond to bells before Grey seals. Grey seals are known to respond to underwater bells used by researchers to study their behavior. Dogs, on the other hand, are typically trained to respond to bells as a conditioned behavior, such as signaling the arrival of food or the start of a training session. There is no evidence that dogs naturally respond to bells before Grey seals.\n",
      "result is error!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "1\n",
      "4\n",
      "{'qid': '1b29d402c3e17cb3b435', 'term': 'Pound sterling', 'description': 'Official currency of the United Kingdom and other territories', 'question': 'Is a pound sterling valuable?', 'answer': False, 'facts': ['A pound sterling is fiat money.', 'Fiat money is backed by government decree and has no intrinsic value.', 'One pound sterling is worth about 1.24 US dollars by May of 2020.'], 'decomposition': ['What is the value of the Pound Sterling based on?', 'Is #1 the material used in making it?'], 'evidence': [[[['Pound sterling-16']], [['Pound sterling-16']]], [[['Pound sterling-1', 'Pound sterling-12']], [['Pound sterling-71']]], [[['Pound sterling-16']], [['One pound (British coin)-3'], 'operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Is a pound sterling valuable?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 65, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "True. A pound sterling is the currency of the United Kingdom and is considered valuable due to its status as a widely recognized and widely traded currency. It is also the currency used in international financial markets for various transactions, including commodities trading and foreign exchange markets. Additionally, the pound sterling is backed by the credit and economic stability of the UK government.\n",
      "result is error!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "1\n",
      "5\n",
      "{'qid': 'c2d2b9ff5a1e682c88dc', 'term': 'Shrimp', 'description': 'Decapod crustaceans', 'question': 'Is shrimp scampi definitely free of plastic?', 'answer': False, 'facts': ['Shrimp scampi is a dish made with shrimp.', 'Shrimp have been found to contain microplastics.', 'Microplastics are plastic material.'], 'decomposition': ['What protein is Shrimp scampi made out of?', 'What have #1 been found to contain?', 'Are #2 free from plastic?'], 'evidence': [[[['Scampi-1']], [['Plastic pollution-31', 'Plastic pollution-48'], 'no_evidence'], ['no_evidence', 'operation']], [[['Fish-92', 'Scampi-2']], [['Microplastics-12']], [['Microplastics-1']]], [[['Scampi-8']], ['no_evidence'], ['no_evidence']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Is shrimp scampi definitely free of plastic?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 70, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False. Shrimp scampi is not definitely free of plastic. While the shrimp itself is typically not made of plastic, the dish often contains butter, garlic, and white wine, which are cooked in a pan or served in a dish that may contain plastic. Additionally, plastic may be present in the packaging or transportation of the shrimp or other ingredients. It's important to be mindful of the potential for plastic contamination in the preparation and serving of food.\n",
      "result is right!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "2\n",
      "6\n",
      "{'qid': '265dd54c248f8b048851', 'term': 'Rede Globo', 'description': 'Brazilian commercial television network', 'question': 'Do the anchors on Rede Globo speak Chinese?', 'answer': False, 'facts': ['Rede Globo is a Brazilian television network.', 'The official language of Brazil is Portuguese.'], 'decomposition': ['What country broadcasts Rede Globo?', 'What is the official language of #1?', 'Is #2 Chinese?'], 'evidence': [[[['Rede Globo-1']], [['Brazil-1']], ['operation']], [[['Rede Globo-1']], [['Brazil-1']], ['operation']], [[['Rede Globo-1']], [['Portuguese language-1']], ['operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Do the anchors on Rede Globo speak Chinese?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 71, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False. The anchors on Rede Globo, a Brazilian television network, do not speak Chinese. They primarily speak Portuguese.\n",
      "result is right!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "3\n",
      "7\n",
      "{'qid': '29bf9aa61ed4124cd84c', 'term': 'Albany, Georgia', 'description': 'City in Georgia, United States', 'question': 'Will the Albany in Georgia reach a hundred thousand occupants before the one in New York?', 'answer': False, 'facts': ['Albany, GA has around 75,000 people', 'Albany, NY has almost 100,000 people'], 'decomposition': ['What is the population of Albany, Georgia?', 'What is the population of Albany, New York?', 'What is the difference between 100,000 and #1?', 'What is the difference between 100,000 and #2?', 'Is #3 smaller than #4?'], 'evidence': [[[['Albany, Georgia-1']], [['Albany, New York-2']], ['operation'], ['operation'], ['operation']], [[['Albany, Georgia-35']], [['Albany, New York-2']], ['operation'], ['operation'], ['operation']], [[['Albany, Georgia-1']], [['Albany, New York-2']], ['operation'], ['operation'], ['operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Will the Albany in Georgia reach a hundred thousand occupants before the one in New York?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 77, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False. The population of Albany, Georgia was estimated to be 77,027 in 2020, according to the United States Census Bureau. Albany, New York, on the other hand, had an estimated population of 97,953 in 2020. Therefore, Albany, New York currently has a larger population than Albany, Georgia. It is unlikely that Georgia's Albany will reach a hundred thousand occupants before New York's Albany does.\n",
      "result is right!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "4\n",
      "8\n",
      "{'qid': 'f231532fe17fd971d1e6', 'term': 'Wonder Woman (2017 film)', 'description': 'American superhero film directed by Patty Jenkins', 'question': 'Is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?', 'answer': True, 'facts': ['The average cost of a US Boeing 737 plane is 1.6 million dollars.', 'Wonder Woman (2017 film) grossed over 800 million dollars at the box office.'], 'decomposition': ['How much does a Boeing 737 cost?', 'How much did the 2017 movie Wonder Woman gross?', 'Is #2 greater than #1?'], 'evidence': [[['no_evidence'], [['Wonder Woman (2017 film)-3']], ['no_evidence', 'operation']], [['no_evidence'], [['Wonder Woman (2017 film)-3']], ['operation']], [[['Boeing 737-13'], 'no_evidence'], [['Wonder Woman (2017 film)-31']], ['operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 84, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False. The box office receipts for the 2017 Wonder Woman film do not cover the cost of a Boeing 737. The film's production budget was reportedly around $149 million, and while it grossed over $821 million worldwide, the costs of filming and marketing the movie would have exceeded the box office revenue. Additionally, there is no evidence that a Boeing 737 was purchased or rented specifically for the film.\n",
      "result is error!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "4\n",
      "9\n",
      "{'qid': 'decb9886ca17f7af209d', 'term': 'Saint Vincent and the Grenadines', 'description': 'Country in the Caribbean', 'question': 'Is the language used in Saint Vincent and the Grenadines rooted in English?', 'answer': True, 'facts': ['The primary language spoken in Saint Vincent and the Grenadines is Vincentian Creole.', 'Vincentian Creole is English-based, with elements of French, Antillean Creole, and indigenous South American and African languages.'], 'decomposition': ['What language is used in Saint Vincent and the Grenadines?', 'Is #1 based in English?'], 'evidence': [[[['Demographics of Saint Vincent and the Grenadines-9']], [['Saint Vincent and the Grenadines-59'], 'operation']], [[['Vincentian Creole-1']], [['Vincentian Creole-1']]], [[['Saint Vincent and the Grenadines-59']], [['Saint Vincent and the Grenadines-59']]]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Is the language used in Saint Vincent and the Grenadines rooted in English?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 75, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer: True. The official language of Saint Vincent and the Grenadines is English. While there are also indigenous languages spoken on the islands, English is used for official business, education, and communication with the outside world.\n",
      "result is right!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "5\n",
      "10\n",
      "{'qid': '622c71f22ddb683c15a8', 'term': 'Casio', 'description': 'Japanese electronics company', 'question': 'Can you buy Casio products at Petco?', 'answer': False, 'facts': ['Casio is a manufacturer of consumer electronics and watches', 'Petco is a chain store that sells pet supplies like food, bowls, litter, toys, cages and grooming equipment'], 'decomposition': ['What kind of products does Casio manufacture?', 'What kind of products does Petco sell?', 'Does #1 overlap with #2?'], 'evidence': [[[['Casio-1']], [['Petco-1']], ['operation']], [[['Casio-1']], [['Petco-1']], ['operation']], [[['Casio-1']], [['Petco-1']], ['operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Can you buy Casio products at Petco?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 68, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False. Petco is a retail store that primarily sells products for pets, such as food, toys, and accessories. Casio is a Japanese electronics manufacturer that produces a wide range of products, including calculators, watches, digital cameras, and other electronic devices. While it's possible that some Petco stores may carry certain Casio products, such as watches, it is not a primary focus of their inventory.\n",
      "result is right!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "6\n",
      "11\n",
      "{'qid': 'b77d2efee37741e44c32', 'term': 'Space Race', 'description': 'Competition between the USSR and the USA to explore space', 'question': 'Did the Space Race use relay batons?', 'answer': False, 'facts': ['The Space Race was a competition between the USA and USSR regarding spaceflight and exploration', 'Relay batons are used in relay races', 'Relay races are athletic track and field events'], 'decomposition': ['What was the Space Race?', 'What are relay batons used for?', 'Is #1 the same as #2?'], 'evidence': [[[['Space Race-1']], [['Relay race-11']], [['Relay race-11', 'Space Race-1'], 'operation']], [[['Space Race-1']], [['Relay race-1']], ['operation']], [[['Space Race-1']], [['Relay race-1']], ['operation']]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Did the Space Race use relay batons?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 67, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False. The Space Race, which was a competition between the United States and the Soviet Union during the Cold War to achieve supremacy in space exploration, did not involve the use of relay batons. Relay batons are used in various sports and competitions to pass a baton from one team member to another during a race. The Space Race was a technological competition in the realm of space exploration, and there was no need for physical batons to be involved.\n",
      "result is right!\n",
      "成功写入字典到文件 'res/Strategyqa_xRAG_Mistral-7B21740479812.json'。\n",
      "7\n",
      "12\n",
      "{'qid': 'd697f6246a7d06e195ee', 'term': 'Deciduous', 'description': 'Trees or shrubs that lose their leaves seasonally', 'question': 'Are Christmas trees dissimilar to deciduous trees?', 'answer': True, 'facts': ['Christmas trees are usually pine trees.', 'Pine trees keep their needles all year round.'], 'decomposition': ['Which kind of trees are commonly used as Christmas trees?', 'Are #1 dissimilar to deciduous trees?'], 'evidence': [[[['Christmas tree-1']], [['Deciduous-1'], 'operation']], [[['Christmas tree-56']], [['Fir-1'], 'operation']], [[['Christmas tree-1']], [['Deciduous-1']]]]}\n",
      "<s>[INST] You are being tested. Follow the instruction below. \n",
      " <unk> Refer to the background document and verify the following claims with \"True\" or \"False\": Are Christmas trees dissimilar to deciduous trees?\n",
      " [/INST] Sure, I'd be happy to help. Here is the answer:\n",
      "torch.Size([1, 69, 4096])\n",
      "特殊 token '<unk>' 的所有位置是: [17]\n",
      "[17]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "background_shot=1\n",
    "correct_num = 0\n",
    "json_QA_list = read_and_parse_QA_list(data_confs[dataset_name]['data_train_path'])\n",
    "for index, qa_dict in enumerate(json_QA_list[:100], 1):\n",
    "    res_dict = {}\n",
    "    print(qa_dict)\n",
    "    qid, facts, question, answer = parse_QA_dict(qa_dict, dataset_name) #解析数据\n",
    "    backgrounds = get_background(facts,dataset_name) #组装backgrounds\n",
    "    prompt = get_task_prompt(question=question,backgrounds=backgrounds,task_type=task_type,rag_type=rag_type,background_shot=background_shot)\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}]\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # input_text = template.format(query=prompt) #根据模型更改template\n",
    "    input_ids = tokenizer(input_text,return_tensors=\"pt\").to(device)\n",
    "    print(input_text)\n",
    "    attention_mask = input_ids.attention_mask\n",
    "    # inputs_embeds = get_tokens_embed(input_text)\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids.input_ids).to(device)\n",
    "    print(inputs_embeds.shape)\n",
    "    if rag_type ==\"xRAG\":\n",
    "        # backgrounds_embs = get_tokens_embed_avg(backgrounds) #获取相应的embedding【1，1，4096】\n",
    "        backgrounds_ids = tokenizer(backgrounds,return_tensors=\"pt\").to(device)\n",
    "        backgrounds_embs = torch.mean(model.model.embed_tokens(backgrounds_ids.input_ids), dim=1, keepdim=True).to(device)\n",
    "        xrag_index = get_xrag_index(input_text)\n",
    "        print(xrag_index)\n",
    "        \n",
    "        inputs_embeds[:,xrag_index[0]:xrag_index[0]+background_shot,:] = backgrounds_embs\n",
    "        # print(backgrounds_embs.shape)\n",
    "        # 使用torch.equal()来判断两个张量是否一致\n",
    "        \n",
    "        \n",
    "        \n",
    "    output = model.generate(inputs_embeds=inputs_embeds,attention_mask = attention_mask, max_new_tokens=max_new_tokens)\n",
    "    # 解码生成的文本\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "\n",
    "    flag = 0\n",
    "    for ans in answer:\n",
    "        if (str(ans).lower() in response.lower()):\n",
    "            res_dict['result'] = 1\n",
    "            flag =1\n",
    "            print(\"result is right!\")\n",
    "            correct_num = correct_num+1\n",
    "            break\n",
    "    if not flag:\n",
    "        res_dict['result'] = 0\n",
    "        print(\"result is error!\")\n",
    "\n",
    "    res_dict['_id'] = qid\n",
    "    res_dict['question'] = question\n",
    "    res_dict['answer'] = answer    \n",
    "    res_dict['response'] = response\n",
    "    res_dict['backgrounds'] = backgrounds\n",
    "    res_dict['input_text']=input_text\n",
    "    write_dict_to_file(res_file, res_dict)\n",
    "    print(correct_num)\n",
    "    print(index)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
